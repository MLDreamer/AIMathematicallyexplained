{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbg4aM8DzlmVKykeum7LC/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MLDreamer/AIMathematicallyexplained/blob/main/DOLA_self_doubt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lLGHRbVZ1C7n"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"microsoft/phi-2\"  # 2.7B parameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Mathematical Consciousness Lab: The Self-Correction Engine\n",
        "A DOLA Playground - Reproduce the mathematics of truthfulness\n",
        "\n",
        "This notebook demonstrates how contrasting early and late transformer layers\n",
        "mathematically eliminates hallucinations. You'll see the equation in action:\n",
        "\n",
        "    log(P_Corrected) = log(P_Late) - \\u03bb \\u00b7 log(P_Early)\n",
        "\n",
        "Author: AI Demystification Series\n",
        "Topic: Decoding by Contrasting Layers (DOLA)\n",
        "\"\"\"\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 0: SETUP & INSTALLATION\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"MATHEMATICAL CONSCIOUSNESS LAB: THE SELF-CORRECTION ENGINE\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nInstalling required libraries...\")\n",
        "print(\"This may take 2-3 minutes on first run.\\n\")\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -q transformers torch accelerate\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"\\u2713 Libraries installed successfully!\\n\")\n",
        "\n",
        "# ==============================================================================\n",
        "# CONFIGURATION: Choose Your Model & Layers\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CONFIGURATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Model selection (using smaller, open-source models for Colab compatibility)\n",
        "# MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"  # 1B parameter model\n",
        "MODEL_NAME = \"microsoft/phi-2\"  # 2.7B parameters\n",
        "# Alternative options:\n",
        "# MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # 1.1B parameters\n",
        "\n",
        "# Layer configuration\n",
        "EARLY_LAYER = 8   # Where factual knowledge lives (typically 10-30% of depth)\n",
        "LATE_LAYER = -1   # Final layer (use -1 for last layer)\n",
        "LAMBDA = 0.7      # Self-correction intensity (0.5 - 1.0 recommended)\n",
        "\n",
        "print(f\"\\n\\u23f9 Configuration:\")\n",
        "print(f\"   Model: {MODEL_NAME}\")\n",
        "print(f\"   Early Layer (Factual): {EARLY_LAYER}\")\n",
        "print(f\"   Late Layer (Final): {LATE_LAYER}\")\n",
        "print(f\"   Lambda (\\u03bb): {LAMBDA}\")\n",
        "print(f\"\\n   Interpretation:\")\n",
        "print(f\"   \\u03bb = 0.0 \\u2192 No correction (standard generation)\")\n",
        "print(f\"   \\u03bb = 0.5 \\u2192 Moderate correction\")\n",
        "print(f\"   \\u03bb = 1.0 \\u2192 Strong correction (maximum factuality)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 1: LOAD THE MODEL & TOKENIZER\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 1: LOADING MODEL\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nLoading {MODEL_NAME}...\")\n",
        "print(\"This may take 1-2 minutes...\\n\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Load model with output_hidden_states enabled (crucial for DOLA!)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    output_hidden_states=True,  # This lets us access intermediate layers\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "total_layers = model.config.num_hidden_layers\n",
        "print(f\"\\u2713 Model loaded successfully!\")\n",
        "print(f\"\\u2713 Total layers: {total_layers}\")\n",
        "print(f\"\\u2713 Vocabulary size: {model.config.vocab_size:,}\")\n",
        "print(f\"\\u2713 Parameters: ~{sum(p.numel() for p in model.parameters()) / 1e9:.2f}B\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 2: THE FACTUAL QUERY\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 2: RUNNING THE FACTUAL QUERY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Test prompts (choose one or add your own)\n",
        "TEST_PROMPTS = [\n",
        "    \"The capital of France is\",\n",
        "    \"The first person to walk on the moon was\",\n",
        "    \"The largest ocean on Earth is the\",\n",
        "    \"Mount Everest is located in\",\n",
        "    \"The speed of light is approximately\"\n",
        "]\n",
        "\n",
        "PROMPT = TEST_PROMPTS[0]  # Change index to test different prompts\n",
        "\n",
        "print(f\"\\n البحث Query: '{PROMPT}'\")\n",
        "print(f\"   Expected answer: Should be factually correct\")\n",
        "print(f\"\\n   Generating forward pass through all {total_layers} layers...\")\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(PROMPT, return_tensors=\"pt\").to(model.device)\n",
        "input_ids = inputs[\"input_ids\"]\n",
        "\n",
        "print(f\"   Tokenized input: {tokenizer.convert_ids_to_tokens(input_ids[0])}\")\n",
        "\n",
        "# Forward pass with hidden states\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs, output_hidden_states=True)\n",
        "\n",
        "    # Extract hidden states from all layers\n",
        "    hidden_states = outputs.hidden_states  # Tuple of (num_layers + 1) tensors\n",
        "\n",
        "    # Get logits (raw scores) from different layers\n",
        "    # We need to manually compute logits from hidden states using the LM head\n",
        "\n",
        "    # Late layer logits (final prediction)\n",
        "    late_hidden = hidden_states[LATE_LAYER]  # Shape: [batch, seq_len, hidden_dim]\n",
        "    late_logits = model.lm_head(late_hidden)  # Shape: [batch, seq_len, vocab_size]\n",
        "\n",
        "    # Early layer logits (factual foundation)\n",
        "    early_hidden = hidden_states[EARLY_LAYER]\n",
        "    early_logits = model.lm_head(early_hidden)\n",
        "\n",
        "    # Focus on the last token's prediction (what comes next)\n",
        "    late_logits_next = late_logits[0, -1, :]  # Shape: [vocab_size]\n",
        "    early_logits_next = early_logits[0, -1, :]  # Shape: [vocab_size]\n",
        "\n",
        "print(f\"\\u2713 Forward pass complete!\")\n",
        "print(f\"\\u2713 Extracted logits from Layer {EARLY_LAYER} (Early) and Layer {LATE_LAYER} (Late)\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 3: EXAMINING THE LAYERED VOTES\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 3: EXTRACTING THE LAYERED VOTES (LOGITS)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "def get_top_k_tokens(logits, k=10):\n",
        "    \"\"\"Get top k tokens and their logits/probabilities\"\"\"\n",
        "    # Convert logits to probabilities\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    # Get top k\n",
        "    top_probs, top_indices = torch.topk(probs, k)\n",
        "\n",
        "    # Decode tokens\n",
        "    top_tokens = [tokenizer.decode([idx]) for idx in top_indices]\n",
        "    top_logits = logits[top_indices].cpu().numpy()\n",
        "    top_probs = top_probs.cpu().numpy()\n",
        "\n",
        "    return top_tokens, top_logits, top_probs\n",
        "\n",
        "print(\"\\n\\U0001f4ca TOP 10 PREDICTIONS FROM EACH LAYER:\\n\")\n",
        "\n",
        "# Early layer predictions\n",
        "print(f\"\\U0001f7e2 EARLY LAYER {EARLY_LAYER} (Factual Foundation):\")\n",
        "print(\"-\" * 70)\n",
        "early_tokens, early_logits_top, early_probs = get_top_k_tokens(early_logits_next, k=10)\n",
        "\n",
        "print(f\"{'Rank':<6} {'Token':<20} {'Logit':<12} {'Probability':<12}\")\n",
        "print(\"-\" * 70)\n",
        "for i, (token, logit, prob) in enumerate(zip(early_tokens, early_logits_top, early_probs), 1):\n",
        "    print(f\"{i:<6} {token:<20} {logit:>10.3f}  {prob:>10.4f} ({prob*100:>5.2f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"\\U0001f534 LATE LAYER (Final Prediction):\")\n",
        "print(\"-\" * 70)\n",
        "late_tokens, late_logits_top, late_probs = get_top_k_tokens(late_logits_next, k=10)\n",
        "\n",
        "print(f\"{'Rank':<6} {'Token':<20} {'Logit':<12} {'Probability':<12}\")\n",
        "print(\"-\" * 70)\n",
        "for i, (token, logit, prob) in enumerate(zip(late_tokens, late_logits_top, late_probs), 1):\n",
        "    print(f\"{i:<6} {token:<20} {logit:>10.3f}  {prob:>10.4f} ({prob*100:>5.2f}%)\")\n",
        "\n",
        "print(\"\\n\\U0001f4a1 OBSERVATION:\")\n",
        "print(\"   Notice how the early layer might be more confident about factual tokens,\")\n",
        "print(\"   while the late layer may distribute probability across more options.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 4: THE MATHEMATICAL SELF-CORRECTION (DOLA)\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 4: IMPLEMENTING DOLA - THE SELF-CORRECTION EQUATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\n\\U0001f4cf Applying the DOLA formula:\")\n",
        "print(f\"   log(P_Corrected) = log(P_Late) - \\u03bb \\u00b7 log(P_Early)\")\n",
        "print(f\"   where \\u03bb = {LAMBDA}\\n\")\n",
        "\n",
        "# Convert logits to log probabilities\n",
        "late_log_probs = torch.log_softmax(late_logits_next, dim=-1)\n",
        "early_log_probs = torch.log_softmax(early_logits_next, dim=-1)\n",
        "\n",
        "# Apply DOLA correction\n",
        "dola_log_probs = late_log_probs - LAMBDA * early_log_probs\n",
        "\n",
        "# Convert back to probabilities\n",
        "dola_probs = torch.softmax(dola_log_probs, dim=-1)\n",
        "\n",
        "# Alternative: Work directly with logits (mathematically equivalent)\n",
        "# dola_logits = late_logits_next - LAMBDA * early_logits_next\n",
        "# dola_probs = torch.softmax(dola_logits, dim=-1)\n",
        "\n",
        "print(\"\\u2713 DOLA correction applied!\")\n",
        "print(f\"   Mathematical operation:\")\n",
        "print(f\"   - Subtracted {LAMBDA} \\u00d7 Early Layer log-probs from Late Layer log-probs\")\n",
        "print(f\"   - Tokens the late layer likes but early layer doesn't \\u2192 PENALIZED\")\n",
        "print(f\"   - Tokens both layers agree on \\u2192 PRESERVED or BOOSTED\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 5: THE GRAND REVEAL\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 5: THE GRAND REVEAL - TRUTH EMERGES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n\\U0001f3af TOP 10 PREDICTIONS AFTER DOLA CORRECTION:\\n\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Get top k from DOLA-corrected distribution\n",
        "top_probs_dola, top_indices_dola = torch.topk(dola_probs, 10)\n",
        "top_tokens_dola = [tokenizer.decode([idx]) for idx in top_indices_dola]\n",
        "top_logits_dola = dola_log_probs[top_indices_dola].cpu().numpy()\n",
        "\n",
        "print(f\"{'Rank':<6} {'Token':<20} {'Log-Prob':<12} {'Probability':<12} {'Change':<10}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for i, (token, log_prob, prob) in enumerate(zip(top_tokens_dola, top_logits_dola, top_probs_dola), 1):\n",
        "    # Find this token's rank in late layer\n",
        "    token_id = top_indices_dola[i-1]\n",
        "    late_prob = late_probs[0] if token == late_tokens[0] else 0.0\n",
        "\n",
        "    # Compute change\n",
        "    change = \"\"\n",
        "    if token in late_tokens[:3]:\n",
        "        late_rank = late_tokens.index(token) + 1\n",
        "        if i < late_rank:\n",
        "            change = f\"\\u2191 from #{late_rank}\"\n",
        "        elif i > late_rank:\n",
        "            change = f\"\\u2193 from #{late_rank}\"\n",
        "        else:\n",
        "            change = f\"= #{late_rank}\"\n",
        "    else:\n",
        "        change = \"NEW\"\n",
        "\n",
        "    prob_val = prob.item()\n",
        "    print(f\"{i:<6} {token:<20} {log_prob:>10.3f}  {prob_val:>10.4f} ({prob_val*100:>5.2f}%)  {change}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 6: COMPARATIVE ANALYSIS\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 6: COMPARATIVE ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n\\U0001f4ca SIDE-BY-SIDE COMPARISON (Top 5):\\n\")\n",
        "print(f\"{'Rank':<6} {'Late Layer':<25} {'Early Layer':<25} {'DOLA Corrected':<25}\")\n",
        "print(\"=\" * 90)\n",
        "\n",
        "for i in range(5):\n",
        "    late_tok = late_tokens[i] if i < len(late_tokens) else \"---\"\n",
        "    early_tok = early_tokens[i] if i < len(early_tokens) else \"---\"\n",
        "    dola_tok = top_tokens_dola[i] if i < len(top_tokens_dola) else \"---\"\n",
        "\n",
        "    late_pct = f\"({late_probs[i]*100:.1f}%)\" if i < len(late_probs) else \"\"\n",
        "    early_pct = f\"({early_probs[i]*100:.1f}%)\" if i < len(early_probs) else \"\"\n",
        "    dola_pct = f\"({top_probs_dola[i]*100:.1f}%)\" if i < len(top_probs_dola) else \"\"\n",
        "\n",
        "    print(f\"{i+1:<6} {late_tok:<15}{late_pct:<10} {early_tok:<15}{early_pct:<10} {dola_tok:<15}{dola_pct:<10}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 7: CHALLENGE YOUR CONCLUSION\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 7: CHALLENGE YOUR CONCLUSION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n\\U0001f914 CRITICAL ANALYSIS:\\n\")\n",
        "\n",
        "# Get the actual winner from each method\n",
        "late_winner = late_tokens[0]\n",
        "early_winner = early_tokens[0]\n",
        "dola_winner = top_tokens_dola[0]\n",
        "\n",
        "print(f\"   Without DOLA (Late Layer): '{late_winner}' wins\")\n",
        "print(f\"   Early Layer says: '{early_winner}' should win\")\n",
        "print(f\"   With DOLA (\\u03bb={LAMBDA}): '{dola_winner}' wins\\n\")\n",
        "\n",
        "if dola_winner == early_winner:\n",
        "    print(\"   \\u2705 RESULT: DOLA shifted prediction toward the factual foundation!\")\n",
        "    print(\"      The mathematical subtraction successfully penalized speculation.\")\n",
        "elif dola_winner == late_winner:\n",
        "    print(\"   \\u26a0\\ufe0f  RESULT: DOLA preserved the late layer's choice\")\n",
        "    print(\"      This suggests both layers agreed, or \\u03bb is too low.\")\n",
        "    print(\"      Try increasing \\u03bb to 0.9 or 1.0 for stronger correction.\")\n",
        "else:\n",
        "    print(\"   \\U0001f3af RESULT: DOLA found a compromise between layers\")\n",
        "    print(\"      The correction balanced factual grounding with context.\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PHILOSOPHICAL QUESTION\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\"\"\n",
        "Did we make the model 'more truthful'? Consider:\n",
        "\n",
        "1. The model's weights didn't change\n",
        "2. We just changed HOW we listen to it\n",
        "3. Both answers were always there, in different layers\n",
        "4. Truth emerged from mathematical contrast, not training\n",
        "\n",
        "Is this genuine truthfulness, or clever probability manipulation?\n",
        "Perhaps truth isn't IN the model\\u2014it's in the tension BETWEEN its layers.\n",
        "\n",
        "The machine didn't learn to stop lying.\n",
        "We learned to hear when it was telling itself the truth all along.\n",
        "\"\"\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 8: INTERACTIVE EXPERIMENTS\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"SECTION 8: YOUR TURN TO EXPERIMENT\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\"\"\n",
        "Try these experiments:\n",
        "\n",
        "1. Change LAMBDA (line 36):\n",
        "   - \\u03bb = 0.0 (no correction)\n",
        "   - \\u03bb = 0.3 (gentle correction)\n",
        "   - \\u03bb = 0.7 (moderate correction)\n",
        "   - \\u03bb = 1.0 (strong correction)\n",
        "   - \\u03bb = 1.5 (aggressive correction)\n",
        "\n",
        "2. Change EARLY_LAYER (line 34):\n",
        "   - Try 5, 10, 15, 20\n",
        "   - Earlier layers = more primitive/factual\n",
        "   - Later layers = more abstract\n",
        "\n",
        "3. Change PROMPT (line 130):\n",
        "   - Try different factual questions\n",
        "   - Try questions with common misconceptions\n",
        "   - Try creative prompts\n",
        "\n",
        "4. Change MODEL (line 24):\n",
        "   - Try different open-source models\n",
        "   - Larger models may show stronger layer differentiation\n",
        "\n",
        "After each change, rerun all cells to see how DOLA responds!\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\\u2713 Notebook complete! The mathematics of self-correction is yours to explore.\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "D_eD0dNL1zI0",
        "outputId": "018b351a-18b2-46fd-ceae-cf71c5213061"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\uXXXX escape (ipython-input-866142177.py, line 114)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-866142177.py\"\u001b[0;36m, line \u001b[0;32m114\u001b[0m\n\u001b[0;31m    print(f\"\\n\\u البحث Query: '{PROMPT}'\")\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\uXXXX escape\n"
          ]
        }
      ]
    }
  ]
}