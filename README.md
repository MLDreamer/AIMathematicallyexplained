AI Mathematically Explained

AI industry claims vs. mathematical reality

What This Is About
This repository proves that modern AI systems are classical mathematics in disguise.
Examples:

RAG = Bayes' theorem (1763)
Transformers = Kernel methods + attention
Neural training = Gradient descent on loss landscapes
Embeddings = Matrix factorization techniques

Each "breakthrough" gets a mathematical proof showing its classical origins.
Structure
/rag-bayesian/          # RAG = Bayes' theorem
/transformer-kernels/   # Attention = kernel methods  
/neural-optimization/   # Training = optimization theory
/embedding-factorization/  # Word vectors = matrix math
/utils/                 # Mathematical tools
Quick Start
bashgit clone https://github.com/[username]/ai-mathematically-explained
cd ai-mathematically-explained
pip install -r requirements.txt
python examples/rag_demo.py
