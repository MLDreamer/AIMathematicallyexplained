{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOynSTVDj/wzg48rEhMRBNn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MLDreamer/AIMathematicallyexplained/blob/main/Value_Matrix_Interactive_Playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EppSE-Mfj5K",
        "outputId": "d71ea38e-dbf0-405d-cec6-947a0deb7f2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Value Matrix as Database: Interactive Experiments\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT 1: Value Matrix as Lossy Compression\n",
            "============================================================\n",
            "\n",
            "Original dimension: 512\n",
            "Compressed dimension: 64\n",
            "Compression ratio: 8.0x\n",
            "\n",
            "Reconstruction MSE: 0.865285\n",
            "Relative error: 86.10%\n",
            "\n",
            "Information loss: 86.10% of original signal\n",
            "\n",
            "Singular value spectrum (top 10):\n",
            "[1.3567705 1.3330604 1.3145422 1.2891326 1.2806014 1.2602961 1.2389673\n",
            " 1.227566  1.2185081 1.2045442]\n",
            "\n",
            "Effective rank (95% variance): 58\n",
            "Out of maximum rank: 64\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT 2: Multi-Head Value Storage\n",
            "============================================================\n",
            "\n",
            "Single head:\n",
            "  V shape: torch.Size([20, 512])\n",
            "  Total dimensions: 512\n",
            "\n",
            "Multi-head (8 heads):\n",
            "  V shape per head: 64\n",
            "  V concatenated shape: torch.Size([20, 512])\n",
            "  Total dimensions: 512\n",
            "\n",
            "Average absolute correlation between heads: 0.026\n",
            "Lower correlation = more diverse information storage\n",
            "✓ Heads are storing diverse information!\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT 3: Information Retrieval via Attention\n",
            "============================================================\n",
            "\n",
            "Value matrix structure:\n",
            "  Tokens 0-1: Person information (dimensions 0-31)\n",
            "  Tokens 2-3: Location information (dimensions 16-47)\n",
            "  Tokens 4-5: Action information (dimensions 32-63)\n",
            "  Tokens 6-7: Noise\n",
            "\n",
            "------------------------------------------------------------\n",
            "Retrieval Results:\n",
            "------------------------------------------------------------\n",
            "\n",
            "Uniform attention (no focus):\n",
            "  Output norm: 2.818\n",
            "  Person dims [0-31]: 1.990\n",
            "  Location dims [16-47]: 2.488\n",
            "  Action dims [32-63]: 1.996\n",
            "  → Blurry, unfocused retrieval\n",
            "\n",
            "Focused on person tokens:\n",
            "  Output norm: 5.091\n",
            "  Person dims [0-31]: 5.091\n",
            "  Location dims [16-47]: 3.600\n",
            "  Action dims [32-63]: 0.000\n",
            "  → Strong person information retrieved!\n",
            "\n",
            "Focused on location tokens:\n",
            "  Output norm: 4.808\n",
            "  Person dims [0-31]: 3.400\n",
            "  Location dims [16-47]: 4.808\n",
            "  Action dims [32-63]: 3.400\n",
            "  → Strong location information retrieved!\n",
            "\n",
            "Mixed focus (person + action):\n",
            "  Output norm: 3.735\n",
            "  Person dims [0-31]: 3.055\n",
            "  Location dims [16-47]: 2.641\n",
            "  Action dims [32-63]: 2.150\n",
            "  → Both person and action information retrieved!\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT 4: KV Cache Memory Requirements\n",
            "============================================================\n",
            "\n",
            "Memory per sequence (in MB):\n",
            "------------------------------------------------------------\n",
            "\n",
            "GPT-2 Small:\n",
            "  Layers: 12, Heads: 12, d_v: 64\n",
            "    Seq len 512: 18.0 MB\n",
            "    Seq len 1024: 36.0 MB\n",
            "    Seq len 2048: 72.0 MB\n",
            "    Seq len 4096: 144.0 MB\n",
            "\n",
            "GPT-2 Large:\n",
            "  Layers: 36, Heads: 20, d_v: 64\n",
            "    Seq len 512: 90.0 MB\n",
            "    Seq len 1024: 180.0 MB\n",
            "    Seq len 2048: 360.0 MB\n",
            "    Seq len 4096: 720.0 MB\n",
            "\n",
            "LLaMA-7B:\n",
            "  Layers: 32, Heads: 32, d_v: 128\n",
            "    Seq len 512: 256.0 MB\n",
            "    Seq len 1024: 512.0 MB\n",
            "    Seq len 2048: 1024.0 MB\n",
            "    Seq len 4096: 2048.0 MB\n",
            "\n",
            "LLaMA-13B:\n",
            "  Layers: 40, Heads: 40, d_v: 128\n",
            "    Seq len 512: 400.0 MB\n",
            "    Seq len 1024: 800.0 MB\n",
            "    Seq len 2048: 1600.0 MB\n",
            "    Seq len 4096: 3200.0 MB\n",
            "\n",
            "============================================================\n",
            "Multi-Query Attention (MQA) Savings:\n",
            "============================================================\n",
            "\n",
            "LLaMA-7B with seq_len=2048:\n",
            "  Standard attention: 1024.0 MB\n",
            "  MQA (1 KV head): 32.0 MB\n",
            "  GQA (4 KV heads): 128.0 MB\n",
            "\n",
            "  MQA reduction: 32x\n",
            "  GQA reduction: 8.0x\n",
            "\n",
            "============================================================\n",
            "EXPERIMENT 5: Learning Value Compression\n",
            "============================================================\n",
            "\n",
            "Training Value matrix to compress color information...\n",
            "Task: Remember RGB color (dims 0-2), forget noise (dims 3-15)\n",
            "  Epoch 0: Loss = 3.020622\n",
            "  Epoch 100: Loss = 0.000230\n",
            "  Epoch 200: Loss = 0.000000\n",
            "  Epoch 300: Loss = 0.000000\n",
            "  Epoch 400: Loss = 0.000000\n",
            "\n",
            "Final loss: 0.000000\n",
            "\n",
            "Learned compression matrix W_V:\n",
            "  Avg weight magnitude for color dims [0-2]: 1.001\n",
            "  Avg weight magnitude for noise dims [3-15]: 0.187\n",
            "  Ratio: 5.35x\n",
            "\n",
            "✓ W_V learned to focus on color information!\n",
            "  It's compressing what matters and discarding noise.\n",
            "\n",
            "============================================================\n",
            "All experiments complete!\n",
            "============================================================\n",
            "\n",
            "Key Takeaways:\n",
            "1. Value matrices compress information lossily\n",
            "2. Multi-head attention creates diverse compressed views\n",
            "3. Attention weights control what information gets retrieved\n",
            "4. KV cache memory is dominated by V (not K)\n",
            "5. Value matrices learn task-specific compression\n",
            "\n",
            "The Value matrix is the database. Attention is just the lookup.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Value Matrix as Learned Database: Interactive Playground\n",
        "Run this in Google Colab to experiment with Value matrices and compression\n",
        "\n",
        "Author: [Your Name]\n",
        "Article: [Link to Medium article]\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Value Matrix as Database: Interactive Experiments\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================================\n",
        "# EXPERIMENT 1: Value Matrix as Lossy Compression\n",
        "# ============================================================================\n",
        "\n",
        "def experiment_1_compression():\n",
        "    \"\"\"\n",
        "    Demonstrate how Value matrix compresses information\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"EXPERIMENT 1: Value Matrix as Lossy Compression\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Original embedding dimension\n",
        "    d_model = 512\n",
        "    # Compressed dimension (per head)\n",
        "    d_v = 64\n",
        "    # Number of tokens\n",
        "    n_tokens = 10\n",
        "\n",
        "    # Create random embeddings (simulating input tokens)\n",
        "    X = torch.randn(n_tokens, d_model)\n",
        "\n",
        "    # Create Value projection matrix\n",
        "    W_V = torch.randn(d_model, d_v) / np.sqrt(d_model)\n",
        "\n",
        "    # Project to Values\n",
        "    V = torch.mm(X, W_V)\n",
        "\n",
        "    # Measure information loss via reconstruction\n",
        "    # Try to reconstruct X from V\n",
        "    W_V_pinv = torch.pinverse(W_V)\n",
        "    X_reconstructed = torch.mm(V, W_V_pinv)\n",
        "\n",
        "    # Calculate reconstruction error\n",
        "    mse = torch.mean((X - X_reconstructed) ** 2).item()\n",
        "    relative_error = mse / torch.mean(X ** 2).item()\n",
        "\n",
        "    print(f\"\\nOriginal dimension: {d_model}\")\n",
        "    print(f\"Compressed dimension: {d_v}\")\n",
        "    print(f\"Compression ratio: {d_model / d_v:.1f}x\")\n",
        "    print(f\"\\nReconstruction MSE: {mse:.6f}\")\n",
        "    print(f\"Relative error: {relative_error:.2%}\")\n",
        "    print(f\"\\nInformation loss: {relative_error:.2%} of original signal\")\n",
        "\n",
        "    # Analyze singular values\n",
        "    U, S, Vh = torch.svd(W_V)\n",
        "\n",
        "    print(f\"\\nSingular value spectrum (top 10):\")\n",
        "    print(S[:10].numpy())\n",
        "\n",
        "    # Calculate effective rank\n",
        "    total_variance = torch.sum(S ** 2)\n",
        "    cumsum = torch.cumsum(S ** 2, dim=0) / total_variance\n",
        "    effective_rank = torch.sum(cumsum < 0.95).item() + 1\n",
        "\n",
        "    print(f\"\\nEffective rank (95% variance): {effective_rank}\")\n",
        "    print(f\"Out of maximum rank: {d_v}\")\n",
        "\n",
        "    return X, V, W_V, S\n",
        "\n",
        "# ============================================================================\n",
        "# EXPERIMENT 2: Multi-Head vs Single-Head Value Storage\n",
        "# ============================================================================\n",
        "\n",
        "def experiment_2_multihead():\n",
        "    \"\"\"\n",
        "    Compare single large V matrix vs multiple smaller V matrices\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"EXPERIMENT 2: Multi-Head Value Storage\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    d_model = 512\n",
        "    n_tokens = 20\n",
        "    X = torch.randn(n_tokens, d_model)\n",
        "\n",
        "    # Single head with large d_v\n",
        "    d_v_single = 512\n",
        "    W_V_single = torch.randn(d_model, d_v_single) / np.sqrt(d_model)\n",
        "    V_single = torch.mm(X, W_V_single)\n",
        "\n",
        "    # Multiple heads with smaller d_v each\n",
        "    n_heads = 8\n",
        "    d_v_per_head = 64  # 8 * 64 = 512 total, same as single head\n",
        "\n",
        "    V_multi = []\n",
        "    for i in range(n_heads):\n",
        "        W_V_head = torch.randn(d_model, d_v_per_head) / np.sqrt(d_model)\n",
        "        V_head = torch.mm(X, W_V_head)\n",
        "        V_multi.append(V_head)\n",
        "\n",
        "    V_multi_concat = torch.cat(V_multi, dim=1)\n",
        "\n",
        "    print(f\"\\nSingle head:\")\n",
        "    print(f\"  V shape: {V_single.shape}\")\n",
        "    print(f\"  Total dimensions: {d_v_single}\")\n",
        "\n",
        "    print(f\"\\nMulti-head ({n_heads} heads):\")\n",
        "    print(f\"  V shape per head: {d_v_per_head}\")\n",
        "    print(f\"  V concatenated shape: {V_multi_concat.shape}\")\n",
        "    print(f\"  Total dimensions: {n_heads * d_v_per_head}\")\n",
        "\n",
        "    # Calculate diversity between heads\n",
        "    correlations = []\n",
        "    for i in range(n_heads):\n",
        "        for j in range(i + 1, n_heads):\n",
        "            # Correlation between head i and j\n",
        "            corr = torch.corrcoef(torch.stack([\n",
        "                V_multi[i].flatten(),\n",
        "                V_multi[j].flatten()\n",
        "            ]))[0, 1].item()\n",
        "            correlations.append(abs(corr))\n",
        "\n",
        "    avg_correlation = np.mean(correlations)\n",
        "\n",
        "    print(f\"\\nAverage absolute correlation between heads: {avg_correlation:.3f}\")\n",
        "    print(f\"Lower correlation = more diverse information storage\")\n",
        "\n",
        "    if avg_correlation < 0.3:\n",
        "        print(\"✓ Heads are storing diverse information!\")\n",
        "    else:\n",
        "        print(\"⚠ Heads might be redundant\")\n",
        "\n",
        "    return V_single, V_multi\n",
        "\n",
        "# ============================================================================\n",
        "# EXPERIMENT 3: Retrieval with Different Attention Patterns\n",
        "# ============================================================================\n",
        "\n",
        "def experiment_3_retrieval():\n",
        "    \"\"\"\n",
        "    Show how attention weights determine what information gets retrieved from V\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"EXPERIMENT 3: Information Retrieval via Attention\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    n_tokens = 8\n",
        "    d_v = 64\n",
        "\n",
        "    # Create Value vectors with distinct patterns\n",
        "    V = torch.zeros(n_tokens, d_v)\n",
        "\n",
        "    # Token 0, 1: \"person\" information (first 32 dims)\n",
        "    V[0, :32] = 1.0\n",
        "    V[1, :32] = 0.8\n",
        "\n",
        "    # Token 2, 3: \"location\" information (middle 32 dims)\n",
        "    V[2, 16:48] = 1.0\n",
        "    V[3, 16:48] = 0.7\n",
        "\n",
        "    # Token 4, 5: \"action\" information (last 32 dims)\n",
        "    V[4, 32:] = 1.0\n",
        "    V[5, 32:] = 0.9\n",
        "\n",
        "    # Rest are noise\n",
        "    V[6:] += torch.randn(2, d_v) * 0.1\n",
        "\n",
        "    print(\"\\nValue matrix structure:\")\n",
        "    print(f\"  Tokens 0-1: Person information (dimensions 0-31)\")\n",
        "    print(f\"  Tokens 2-3: Location information (dimensions 16-47)\")\n",
        "    print(f\"  Tokens 4-5: Action information (dimensions 32-63)\")\n",
        "    print(f\"  Tokens 6-7: Noise\")\n",
        "\n",
        "    # Scenario 1: Uniform attention (no focus)\n",
        "    alpha_uniform = torch.ones(n_tokens) / n_tokens\n",
        "    output_uniform = torch.mv(V.t(), alpha_uniform)\n",
        "\n",
        "    # Scenario 2: Focus on person tokens\n",
        "    alpha_person = torch.zeros(n_tokens)\n",
        "    alpha_person[0:2] = 0.5\n",
        "    output_person = torch.mv(V.t(), alpha_person)\n",
        "\n",
        "    # Scenario 3: Focus on location tokens\n",
        "    alpha_location = torch.zeros(n_tokens)\n",
        "    alpha_location[2:4] = 0.5\n",
        "    output_location = torch.mv(V.t(), alpha_location)\n",
        "\n",
        "    # Scenario 4: Mixed focus (person + action)\n",
        "    alpha_mixed = torch.zeros(n_tokens)\n",
        "    alpha_mixed[0:2] = 0.3\n",
        "    alpha_mixed[4:6] = 0.2\n",
        "    output_mixed = torch.mv(V.t(), alpha_mixed)\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 60)\n",
        "    print(\"Retrieval Results:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    print(f\"\\nUniform attention (no focus):\")\n",
        "    print(f\"  Output norm: {torch.norm(output_uniform):.3f}\")\n",
        "    print(f\"  Person dims [0-31]: {torch.norm(output_uniform[:32]):.3f}\")\n",
        "    print(f\"  Location dims [16-47]: {torch.norm(output_uniform[16:48]):.3f}\")\n",
        "    print(f\"  Action dims [32-63]: {torch.norm(output_uniform[32:]):.3f}\")\n",
        "    print(\"  → Blurry, unfocused retrieval\")\n",
        "\n",
        "    print(f\"\\nFocused on person tokens:\")\n",
        "    print(f\"  Output norm: {torch.norm(output_person):.3f}\")\n",
        "    print(f\"  Person dims [0-31]: {torch.norm(output_person[:32]):.3f}\")\n",
        "    print(f\"  Location dims [16-47]: {torch.norm(output_person[16:48]):.3f}\")\n",
        "    print(f\"  Action dims [32-63]: {torch.norm(output_person[32:]):.3f}\")\n",
        "    print(\"  → Strong person information retrieved!\")\n",
        "\n",
        "    print(f\"\\nFocused on location tokens:\")\n",
        "    print(f\"  Output norm: {torch.norm(output_location):.3f}\")\n",
        "    print(f\"  Person dims [0-31]: {torch.norm(output_location[:32]):.3f}\")\n",
        "    print(f\"  Location dims [16-47]: {torch.norm(output_location[16:48]):.3f}\")\n",
        "    print(f\"  Action dims [32-63]: {torch.norm(output_location[32:]):.3f}\")\n",
        "    print(\"  → Strong location information retrieved!\")\n",
        "\n",
        "    print(f\"\\nMixed focus (person + action):\")\n",
        "    print(f\"  Output norm: {torch.norm(output_mixed):.3f}\")\n",
        "    print(f\"  Person dims [0-31]: {torch.norm(output_mixed[:32]):.3f}\")\n",
        "    print(f\"  Location dims [16-47]: {torch.norm(output_mixed[16:48]):.3f}\")\n",
        "    print(f\"  Action dims [32-63]: {torch.norm(output_mixed[32:]):.3f}\")\n",
        "    print(\"  → Both person and action information retrieved!\")\n",
        "\n",
        "    return V, [alpha_uniform, alpha_person, alpha_location, alpha_mixed]\n",
        "\n",
        "# ============================================================================\n",
        "# EXPERIMENT 4: KV Cache Memory Calculation\n",
        "# ============================================================================\n",
        "\n",
        "def experiment_4_kv_cache():\n",
        "    \"\"\"\n",
        "    Calculate actual KV cache memory requirements\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"EXPERIMENT 4: KV Cache Memory Requirements\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Model configurations\n",
        "    configs = {\n",
        "        \"GPT-2 Small\": {\"n_layers\": 12, \"n_heads\": 12, \"d_v\": 64},\n",
        "        \"GPT-2 Large\": {\"n_layers\": 36, \"n_heads\": 20, \"d_v\": 64},\n",
        "        \"LLaMA-7B\": {\"n_layers\": 32, \"n_heads\": 32, \"d_v\": 128},\n",
        "        \"LLaMA-13B\": {\"n_layers\": 40, \"n_heads\": 40, \"d_v\": 128},\n",
        "    }\n",
        "\n",
        "    sequence_lengths = [512, 1024, 2048, 4096]\n",
        "    precision_bytes = 2  # fp16\n",
        "\n",
        "    print(\"\\nMemory per sequence (in MB):\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for model_name, config in configs.items():\n",
        "        print(f\"\\n{model_name}:\")\n",
        "        print(f\"  Layers: {config['n_layers']}, Heads: {config['n_heads']}, d_v: {config['d_v']}\")\n",
        "\n",
        "        for seq_len in sequence_lengths:\n",
        "            # K and V cache: 2 * seq_len * d_v * n_heads * n_layers * precision\n",
        "            memory_bytes = 2 * seq_len * config['d_v'] * config['n_heads'] * config['n_layers'] * precision_bytes\n",
        "            memory_mb = memory_bytes / (1024 ** 2)\n",
        "\n",
        "            print(f\"    Seq len {seq_len}: {memory_mb:.1f} MB\")\n",
        "\n",
        "    # Multi-Query Attention comparison\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Multi-Query Attention (MQA) Savings:\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    model_name = \"LLaMA-7B\"\n",
        "    config = configs[model_name]\n",
        "    seq_len = 2048\n",
        "\n",
        "    # Standard attention\n",
        "    standard_memory = 2 * seq_len * config['d_v'] * config['n_heads'] * config['n_layers'] * precision_bytes\n",
        "\n",
        "    # MQA: only 1 KV head shared across all query heads\n",
        "    mqa_memory = 2 * seq_len * config['d_v'] * 1 * config['n_layers'] * precision_bytes\n",
        "\n",
        "    # GQA: 4 KV heads (grouped)\n",
        "    n_kv_heads_gqa = 4\n",
        "    gqa_memory = 2 * seq_len * config['d_v'] * n_kv_heads_gqa * config['n_layers'] * precision_bytes\n",
        "\n",
        "    print(f\"\\n{model_name} with seq_len={seq_len}:\")\n",
        "    print(f\"  Standard attention: {standard_memory / (1024**2):.1f} MB\")\n",
        "    print(f\"  MQA (1 KV head): {mqa_memory / (1024**2):.1f} MB\")\n",
        "    print(f\"  GQA ({n_kv_heads_gqa} KV heads): {gqa_memory / (1024**2):.1f} MB\")\n",
        "    print(f\"\\n  MQA reduction: {config['n_heads']}x\")\n",
        "    print(f\"  GQA reduction: {config['n_heads'] / n_kv_heads_gqa:.1f}x\")\n",
        "\n",
        "# ============================================================================\n",
        "# EXPERIMENT 5: Learning Value Compression\n",
        "# ============================================================================\n",
        "\n",
        "def experiment_5_learning():\n",
        "    \"\"\"\n",
        "    Simulate how Value matrix learns to compress information\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"EXPERIMENT 5: Learning Value Compression\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Create a toy task: retrieve color information\n",
        "    # Embeddings encode: [color_r, color_g, color_b, shape, size, ...]\n",
        "\n",
        "    d_model = 16\n",
        "    d_v = 4  # Compress to 4 dimensions\n",
        "    n_samples = 100\n",
        "\n",
        "    # Generate data\n",
        "    # First 3 dims: RGB color (what we want to remember)\n",
        "    # Rest: noise (what we want to forget)\n",
        "    X = torch.randn(n_samples, d_model)\n",
        "    X[:, :3] = torch.randn(n_samples, 3) * 2  # Stronger signal in color dims\n",
        "\n",
        "    # Target: retrieve the color (first 3 dims)\n",
        "    Y = X[:, :3]\n",
        "\n",
        "    # Initialize Value projection\n",
        "    W_V = nn.Parameter(torch.randn(d_model, d_v) / np.sqrt(d_model))\n",
        "    W_out = nn.Parameter(torch.randn(d_v, 3) / np.sqrt(d_v))\n",
        "\n",
        "    optimizer = torch.optim.Adam([W_V, W_out], lr=0.01)\n",
        "\n",
        "    print(\"\\nTraining Value matrix to compress color information...\")\n",
        "    print(\"Task: Remember RGB color (dims 0-2), forget noise (dims 3-15)\")\n",
        "\n",
        "    losses = []\n",
        "    for epoch in range(500):\n",
        "        # Forward pass\n",
        "        V = torch.mm(X, W_V)  # Compress\n",
        "        Y_pred = torch.mm(V, W_out)  # Reconstruct color\n",
        "\n",
        "        # Loss: MSE between true color and reconstructed color\n",
        "        loss = torch.mean((Y - Y_pred) ** 2)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"  Epoch {epoch}: Loss = {loss.item():.6f}\")\n",
        "\n",
        "    print(f\"\\nFinal loss: {losses[-1]:.6f}\")\n",
        "\n",
        "    # Analyze what W_V learned\n",
        "    W_V_learned = W_V.detach()\n",
        "\n",
        "    # Check if W_V focuses on color dimensions\n",
        "    color_weights = torch.norm(W_V_learned[:3, :], dim=1)\n",
        "    noise_weights = torch.norm(W_V_learned[3:, :], dim=1)\n",
        "\n",
        "    print(f\"\\nLearned compression matrix W_V:\")\n",
        "    print(f\"  Avg weight magnitude for color dims [0-2]: {torch.mean(color_weights):.3f}\")\n",
        "    print(f\"  Avg weight magnitude for noise dims [3-15]: {torch.mean(noise_weights):.3f}\")\n",
        "    print(f\"  Ratio: {torch.mean(color_weights) / torch.mean(noise_weights):.2f}x\")\n",
        "\n",
        "    if torch.mean(color_weights) > torch.mean(noise_weights) * 1.5:\n",
        "        print(\"\\n✓ W_V learned to focus on color information!\")\n",
        "        print(\"  It's compressing what matters and discarding noise.\")\n",
        "    else:\n",
        "        print(\"\\n⚠ W_V didn't learn clear focus\")\n",
        "\n",
        "    return W_V_learned, losses\n",
        "\n",
        "# ============================================================================\n",
        "# RUN ALL EXPERIMENTS\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Experiment 1\n",
        "    X, V, W_V, S = experiment_1_compression()\n",
        "\n",
        "    # Experiment 2\n",
        "    V_single, V_multi = experiment_2_multihead()\n",
        "\n",
        "    # Experiment 3\n",
        "    V_retrieval, attention_patterns = experiment_3_retrieval()\n",
        "\n",
        "    # Experiment 4\n",
        "    experiment_4_kv_cache()\n",
        "\n",
        "    # Experiment 5\n",
        "    W_V_learned, losses = experiment_5_learning()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"All experiments complete!\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\nKey Takeaways:\")\n",
        "    print(\"1. Value matrices compress information lossily\")\n",
        "    print(\"2. Multi-head attention creates diverse compressed views\")\n",
        "    print(\"3. Attention weights control what information gets retrieved\")\n",
        "    print(\"4. KV cache memory is dominated by V (not K)\")\n",
        "    print(\"5. Value matrices learn task-specific compression\")\n",
        "    print(\"\\nThe Value matrix is the database. Attention is just the lookup.\")"
      ]
    }
  ]
}