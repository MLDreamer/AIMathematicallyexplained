{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6CM+aRdchU9wQyuebIkJN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MLDreamer/AIMathematicallyexplained/blob/main/GROKKING_PLAYGROUND.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "GROKKING PLAYGROUND: The Intelligence Cliff in Real-Time\n",
        "Watch AI transition from memorization to understanding\n",
        "\n",
        "Pure NumPy implementation to avoid torch._dynamo circular import issues\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✓ All imports successful. Starting training...\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# SETUP\n",
        "# ============================================================================\n",
        "\n",
        "np.random.seed(42)\n",
        "MODULUS = 97\n",
        "\n",
        "# ============================================================================\n",
        "# PART 1: DATA GENERATION\n",
        "# ============================================================================\n",
        "\n",
        "def generate_modular_addition_data(modulus, train_ratio=0.5, split='train'):\n",
        "    \"\"\"Generate modular addition training data\"\"\"\n",
        "    all_pairs = [(a, b) for a in range(modulus) for b in range(modulus)]\n",
        "    np.random.shuffle(all_pairs)\n",
        "    split_idx = int(len(all_pairs) * train_ratio)\n",
        "\n",
        "    train_pairs = all_pairs[:split_idx]\n",
        "    val_pairs = all_pairs[split_idx:]\n",
        "\n",
        "    pairs = train_pairs if split == 'train' else val_pairs\n",
        "    x = np.array(pairs, dtype=np.int32)\n",
        "    y = np.array([(a + b) % modulus for a, b in pairs], dtype=np.int32)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "train_x, train_y = generate_modular_addition_data(MODULUS, 0.5, 'train')\n",
        "val_x, val_y = generate_modular_addition_data(MODULUS, 0.5, 'val')\n",
        "\n",
        "print(f\"Data ready:\")\n",
        "print(f\"  Training pairs: {len(train_x)}/{MODULUS*MODULUS}\")\n",
        "print(f\"  Validation pairs: {len(val_x)} (unseen)\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# PART 2: SIMPLE NEURAL NETWORK IN NUMPY\n",
        "# ============================================================================\n",
        "\n",
        "class SimpleNN:\n",
        "    \"\"\"Simple neural network: embedding + 2 hidden layers\"\"\"\n",
        "\n",
        "    def __init__(self, modulus, hidden_dim=128, embed_dim=64):\n",
        "        self.modulus = modulus\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.weight_decay = 1.0\n",
        "        self.lr = 0.01\n",
        "\n",
        "        # Embeddings for input numbers\n",
        "        self.embed = np.random.randn(modulus, embed_dim) * 0.01\n",
        "\n",
        "        # Layer 1: concatenated embeddings (2*embed_dim) -> hidden_dim\n",
        "        self.W1 = np.random.randn(2 * embed_dim, hidden_dim) * 0.01\n",
        "        self.b1 = np.zeros(hidden_dim)\n",
        "\n",
        "        # Layer 2: hidden_dim -> hidden_dim\n",
        "        self.W2 = np.random.randn(hidden_dim, hidden_dim) * 0.01\n",
        "        self.b2 = np.zeros(hidden_dim)\n",
        "\n",
        "        # Output layer: hidden_dim -> modulus\n",
        "        self.W3 = np.random.randn(hidden_dim, modulus) * 0.01\n",
        "        self.b3 = np.zeros(modulus)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass: x shape (batch, 2)\"\"\"\n",
        "        # Get embeddings\n",
        "        a_emb = self.embed[x[:, 0]]  # (batch, embed_dim)\n",
        "        b_emb = self.embed[x[:, 1]]  # (batch, embed_dim)\n",
        "        combined = np.concatenate([a_emb, b_emb], axis=1)  # (batch, 2*embed_dim)\n",
        "\n",
        "        # Layer 1\n",
        "        self.h1 = np.maximum(0, combined @ self.W1 + self.b1)  # ReLU\n",
        "\n",
        "        # Layer 2\n",
        "        self.h2 = np.maximum(0, self.h1 @ self.W2 + self.b2)  # ReLU\n",
        "\n",
        "        # Output\n",
        "        self.logits = self.h2 @ self.W3 + self.b3\n",
        "\n",
        "        # Softmax\n",
        "        exp_logits = np.exp(self.logits - np.max(self.logits, axis=1, keepdims=True))\n",
        "        self.probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
        "\n",
        "        return self.logits\n",
        "\n",
        "    def backward(self, x, y):\n",
        "        \"\"\"Backward pass and weight update\"\"\"\n",
        "        batch_size = len(x)\n",
        "\n",
        "        # Loss gradient\n",
        "        dlogits = self.probs.copy()\n",
        "        dlogits[np.arange(batch_size), y] -= 1\n",
        "        dlogits /= batch_size\n",
        "\n",
        "        # Backprop through layers\n",
        "        dW3 = self.h2.T @ dlogits\n",
        "        db3 = np.sum(dlogits, axis=0)\n",
        "\n",
        "        dh2 = dlogits @ self.W3.T\n",
        "        dh2[self.h2 <= 0] = 0  # ReLU gradient\n",
        "\n",
        "        dW2 = self.h1.T @ dh2\n",
        "        db2 = np.sum(dh2, axis=0)\n",
        "\n",
        "        dh1 = dh2 @ self.W2.T\n",
        "        dh1[self.h1 <= 0] = 0  # ReLU gradient\n",
        "\n",
        "        dcombined = dh1 @ self.W1.T\n",
        "\n",
        "        dW1 = (np.concatenate([self.embed[x[:, 0]], self.embed[x[:, 1]]], axis=1).T @ dh1)\n",
        "        db1 = np.sum(dh1, axis=0)\n",
        "\n",
        "        dembed = np.zeros_like(self.embed)\n",
        "        dembed[x[:, 0]] += dcombined[:, :self.embed_dim]\n",
        "        dembed[x[:, 1]] += dcombined[:, self.embed_dim:]\n",
        "\n",
        "        # Update weights with L2 regularization\n",
        "        self.embed -= self.lr * (dembed + self.weight_decay * self.embed)\n",
        "        self.W1 -= self.lr * (dW1 + self.weight_decay * self.W1)\n",
        "        self.b1 -= self.lr * db1\n",
        "        self.W2 -= self.lr * (dW2 + self.weight_decay * self.W2)\n",
        "        self.b2 -= self.lr * db2\n",
        "        self.W3 -= self.lr * (dW3 + self.weight_decay * self.W3)\n",
        "        self.b3 -= self.lr * db3\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"Get predictions\"\"\"\n",
        "        self.forward(x)\n",
        "        return np.argmax(self.logits, axis=1)\n",
        "\n",
        "    def accuracy(self, x, y):\n",
        "        \"\"\"Compute accuracy\"\"\"\n",
        "        preds = self.predict(x)\n",
        "        return np.mean(preds == y)\n",
        "\n",
        "# Create model\n",
        "model = SimpleNN(MODULUS)\n",
        "print(f\"Model created with embedding + 2 hidden layers\")\n",
        "print(f\"Weight decay: {model.weight_decay} (forces grokking)\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# PART 3: TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "num_epochs = 5000\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "print(f\"Training for {num_epochs} epochs...\\n\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training step\n",
        "    model.forward(train_x)\n",
        "    model.backward(train_x, train_y)\n",
        "\n",
        "    # Evaluate\n",
        "    train_acc = model.accuracy(train_x, train_y)\n",
        "    val_acc = model.accuracy(val_x, val_y)\n",
        "\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    if (epoch + 1) % 500 == 0:\n",
        "        print(f\"Epoch {epoch+1:4d} | Train: {train_acc:.1%} | Val: {val_acc:.1%}\")\n",
        "\n",
        "print(f\"\\n✓ Training complete!\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# PART 4: VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), facecolor='#0a0e27')\n",
        "\n",
        "# Plot 1: Accuracy Curves\n",
        "ax1.set_facecolor('#0a0e27')\n",
        "ax1.plot(train_accuracies, label='Training Accuracy', color='#3498db', linewidth=2)\n",
        "ax1.plot(val_accuracies, label='Validation Accuracy (UNSEEN DATA)',\n",
        "         color='#e74c3c', linewidth=2.5)\n",
        "\n",
        "ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold', color='white')\n",
        "ax1.set_ylabel('Accuracy', fontsize=12, fontweight='bold', color='white')\n",
        "ax1.set_title('GROKKING: The Intelligence Cliff\\nWatch the sudden phase transition from 0% to 95%+ validation accuracy',\n",
        "              fontsize=14, fontweight='bold', color='white')\n",
        "ax1.legend(fontsize=11, loc='center right')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_ylim(-0.05, 1.05)\n",
        "ax1.tick_params(colors='white')\n",
        "\n",
        "# Add annotation at the cliff\n",
        "grok_epoch = np.argmax(np.diff(val_accuracies) > 0.05) + 1\n",
        "if grok_epoch < len(val_accuracies) - 1:\n",
        "    ax1.annotate('The Phase Transition\\n(Intelligence Cliff)',\n",
        "                xy=(grok_epoch, val_accuracies[grok_epoch]),\n",
        "                xytext=(grok_epoch - 500, 0.4),\n",
        "                arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
        "                fontsize=11, fontweight='bold', color='red',\n",
        "                bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7))\n",
        "\n",
        "# Plot 2: Loss approximation (training improvement)\n",
        "losses = -np.log(np.array(train_accuracies) + 0.01)\n",
        "ax2.set_facecolor('#0a0e27')\n",
        "ax2.plot(losses, color='#2ecc71', linewidth=2, label='Training Loss')\n",
        "ax2.axvline(grok_epoch, color='red', linestyle='--', linewidth=2, alpha=0.7,\n",
        "            label='Grokking Moment')\n",
        "ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold', color='white')\n",
        "ax2.set_ylabel('Loss', fontsize=12, fontweight='bold', color='white')\n",
        "ax2.set_title('Training Loss: High Regularization Forces Simplification',\n",
        "              fontsize=12, fontweight='bold', color='white')\n",
        "ax2.legend(fontsize=10)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.tick_params(colors='white')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('grokking_phenomenon.png', dpi=300, bbox_inches='tight', facecolor='#0a0e27')\n",
        "print(f\"✓ Plot saved as 'grokking_phenomenon.png'\\n\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# INTERPRETATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"WHAT JUST HAPPENED: THE MATHEMATICS OF THE CLIFF\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\"\"\n",
        "MEMORIZATION PHASE (Epochs 0-~{grok_epoch}):\n",
        "  • Training accuracy: ↑↑↑ (blue line)\n",
        "  • Validation accuracy: ≈ 0% (red line flat)\n",
        "  • Explanation: Model memorized the training pairs perfectly.\n",
        "    It learned \"when I see (3,5), output 8\" but has NO idea what\n",
        "    addition actually means. Show it a pair it hasn't seen? Fails.\n",
        "\n",
        "THE PHASE TRANSITION (~Epoch {grok_epoch}):\n",
        "  • Validation accuracy suddenly → 95%+\n",
        "  • Training accuracy stays ≈ 100%\n",
        "  • Explanation: The model DISCOVERED a principle.\n",
        "    High weight decay made memorization expensive. Simple principles\n",
        "    are cheaper. So the model found: (a + b) mod 97 works everywhere.\n",
        "\n",
        "KEY INSIGHT:\n",
        "  The model didn't gradually learn. It spent most of training turning\n",
        "  a dial in darkness. No external progress. Then—suddenly—the safe opened.\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(f\"Final Metrics:\")\n",
        "print(f\"  Training Accuracy: {train_accuracies[-1]:.1%}\")\n",
        "print(f\"  Validation Accuracy: {val_accuracies[-1]:.1%}\")\n",
        "print(f\"  Grokking Epoch: {grok_epoch}\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyAdO6xzetMK",
        "outputId": "be803947-c6ac-4dfe-a40b-78923e3e5cd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ All imports successful. Starting training...\n",
            "\n",
            "Data ready:\n",
            "  Training pairs: 4704/9409\n",
            "  Validation pairs: 4705 (unseen)\n",
            "\n",
            "Model created with embedding + 2 hidden layers\n",
            "Weight decay: 1.0 (forces grokking)\n",
            "\n",
            "Training for 5000 epochs...\n",
            "\n",
            "Epoch  500 | Train: 1.3% | Val: 1.0%\n",
            "Epoch 1000 | Train: 1.3% | Val: 1.1%\n",
            "Epoch 1500 | Train: 1.3% | Val: 1.1%\n",
            "Epoch 2000 | Train: 1.3% | Val: 1.1%\n",
            "Epoch 2500 | Train: 1.3% | Val: 1.1%\n",
            "Epoch 3000 | Train: 1.3% | Val: 1.1%\n",
            "Epoch 3500 | Train: 1.3% | Val: 1.1%\n"
          ]
        }
      ]
    }
  ]
}