{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production RAG Implementation Guide\n",
    "\n",
    "This notebook contains all code referenced in the Production RAG Bible.\n",
    "\n",
    "**Setup**: Run the installation cell, then jump to the pattern you need.\n",
    "\n",
    "**Structure**:\n",
    "1. Installation & Setup\n",
    "2. Evaluation Framework\n",
    "3. RAG Pattern Implementations\n",
    "4. Benchmarking Tools\n",
    "5. Production Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core dependencies\n",
    "!pip install -q openai anthropic\n",
    "!pip install -q langchain langchain-community langchain-openai\n",
    "!pip install -q chromadb sentence-transformers\n",
    "!pip install -q numpy pandas scikit-learn\n",
    "!pip install -q tqdm python-dotenv\n",
    "!pip install -q ragas  # For eval metrics\n",
    "\n",
    "# Optional: For GraphRAG\n",
    "# !pip install -q neo4j spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "# Optional: For reranking\n",
    "# !pip install -q cohere voyageai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# LLM clients\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "\n",
    "# Vector store\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Set your API keys\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Initialize clients\n",
    "openai_client = OpenAI()\n",
    "anthropic_client = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation Framework\n",
    "\n",
    "**Critical**: Build this first. Every pattern depends on measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EvalExample:\n",
    "    \"\"\"Single evaluation example\"\"\"\n",
    "    query: str\n",
    "    ground_truth_answer: str\n",
    "    relevant_doc_ids: List[str]  # IDs of documents that should be retrieved\n",
    "    category: str = \"general\"  # For segmented analysis\n",
    "\n",
    "@dataclass\n",
    "class RetrievalMetrics:\n",
    "    \"\"\"Retrieval evaluation results\"\"\"\n",
    "    precision_at_k: Dict[int, float]  # {5: 0.6, 10: 0.5}\n",
    "    recall_at_k: Dict[int, float]\n",
    "    mrr: float\n",
    "    num_queries: int\n",
    "\n",
    "@dataclass\n",
    "class AnswerMetrics:\n",
    "    \"\"\"Answer evaluation results\"\"\"\n",
    "    correctness: float\n",
    "    faithfulness: float\n",
    "    relevance: float\n",
    "    num_queries: int\n",
    "\n",
    "@dataclass\n",
    "class LatencyMetrics:\n",
    "    \"\"\"Latency measurements\"\"\"\n",
    "    p50: float\n",
    "    p95: float\n",
    "    p99: float\n",
    "    mean: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalEvaluator:\n",
    "    \"\"\"Evaluate retrieval quality\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def precision_at_k(retrieved_ids: List[str], \n",
    "                       relevant_ids: List[str], \n",
    "                       k: int) -> float:\n",
    "        \"\"\"Precision@K: What % of top-K retrieved docs are relevant?\"\"\"\n",
    "        if k == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        retrieved_at_k = set(retrieved_ids[:k])\n",
    "        relevant_set = set(relevant_ids)\n",
    "        \n",
    "        num_relevant = len(retrieved_at_k.intersection(relevant_set))\n",
    "        return num_relevant / k\n",
    "    \n",
    "    @staticmethod\n",
    "    def recall_at_k(retrieved_ids: List[str], \n",
    "                    relevant_ids: List[str], \n",
    "                    k: int) -> float:\n",
    "        \"\"\"Recall@K: What % of all relevant docs are in top-K?\"\"\"\n",
    "        if len(relevant_ids) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        retrieved_at_k = set(retrieved_ids[:k])\n",
    "        relevant_set = set(relevant_ids)\n",
    "        \n",
    "        num_found = len(retrieved_at_k.intersection(relevant_set))\n",
    "        return num_found / len(relevant_ids)\n",
    "    \n",
    "    @staticmethod\n",
    "    def reciprocal_rank(retrieved_ids: List[str], \n",
    "                       relevant_ids: List[str]) -> float:\n",
    "        \"\"\"Reciprocal Rank: 1 / position of first relevant doc\"\"\"\n",
    "        relevant_set = set(relevant_ids)\n",
    "        \n",
    "        for idx, doc_id in enumerate(retrieved_ids, 1):\n",
    "            if doc_id in relevant_set:\n",
    "                return 1.0 / idx\n",
    "        \n",
    "        return 0.0\n",
    "    \n",
    "    def evaluate(self, \n",
    "                 eval_examples: List[EvalExample],\n",
    "                 retrieval_fn,\n",
    "                 k_values: List[int] = [5, 10]) -> RetrievalMetrics:\n",
    "        \"\"\"Run full retrieval evaluation\"\"\"\n",
    "        \n",
    "        precision_scores = {k: [] for k in k_values}\n",
    "        recall_scores = {k: [] for k in k_values}\n",
    "        rr_scores = []\n",
    "        \n",
    "        for example in eval_examples:\n",
    "            # Get retrieved doc IDs from your system\n",
    "            retrieved_ids = retrieval_fn(example.query)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            for k in k_values:\n",
    "                p = self.precision_at_k(retrieved_ids, example.relevant_doc_ids, k)\n",
    "                r = self.recall_at_k(retrieved_ids, example.relevant_doc_ids, k)\n",
    "                precision_scores[k].append(p)\n",
    "                recall_scores[k].append(r)\n",
    "            \n",
    "            rr = self.reciprocal_rank(retrieved_ids, example.relevant_doc_ids)\n",
    "            rr_scores.append(rr)\n",
    "        \n",
    "        return RetrievalMetrics(\n",
    "            precision_at_k={k: np.mean(scores) for k, scores in precision_scores.items()},\n",
    "            recall_at_k={k: np.mean(scores) for k, scores in recall_scores.items()},\n",
    "            mrr=np.mean(rr_scores),\n",
    "            num_queries=len(eval_examples)\n",
    "        )\n",
    "\n",
    "# Usage example\n",
    "# evaluator = RetrievalEvaluator()\n",
    "# metrics = evaluator.evaluate(eval_examples, my_retrieval_function)\n",
    "# print(f\"Precision@5: {metrics.precision_at_k[5]:.3f}\")\n",
    "# print(f\"Recall@10: {metrics.recall_at_k[10]:.3f}\")\n",
    "# print(f\"MRR: {metrics.mrr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerEvaluator:\n",
    "    \"\"\"Evaluate answer quality using LLM-as-Judge\"\"\"\n",
    "    \n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "    \n",
    "    def evaluate_correctness(self, \n",
    "                            query: str, \n",
    "                            generated_answer: str, \n",
    "                            ground_truth: str) -> float:\n",
    "        \"\"\"Score: Does the answer match ground truth?\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"You are evaluating answer correctness.\n",
    "\n",
    "Query: {query}\n",
    "Ground Truth Answer: {ground_truth}\n",
    "Generated Answer: {generated_answer}\n",
    "\n",
    "Does the generated answer convey the same information as the ground truth?\n",
    "Respond with a score from 0.0 to 1.0, where:\n",
    "- 1.0 = Perfect match (same facts, same meaning)\n",
    "- 0.7-0.9 = Mostly correct (minor differences)\n",
    "- 0.4-0.6 = Partially correct (missing key info)\n",
    "- 0.0-0.3 = Incorrect or contradictory\n",
    "\n",
    "Respond with ONLY a number between 0.0 and 1.0.\"\"\"\n",
    "        \n",
    "        response = self.client.messages.create(\n",
    "            model=\"claude-sonnet-4-20250514\",\n",
    "            max_tokens=10,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            score = float(response.content[0].text.strip())\n",
    "            return max(0.0, min(1.0, score))\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_faithfulness(self, \n",
    "                             generated_answer: str, \n",
    "                             retrieved_docs: List[str]) -> float:\n",
    "        \"\"\"Score: Are all claims in the answer supported by the docs?\"\"\"\n",
    "        \n",
    "        context = \"\\n\\n\".join(retrieved_docs)\n",
    "        \n",
    "        prompt = f\"\"\"You are evaluating answer faithfulness.\n",
    "\n",
    "Retrieved Context:\n",
    "{context}\n",
    "\n",
    "Generated Answer:\n",
    "{generated_answer}\n",
    "\n",
    "Are ALL claims in the answer supported by the context?\n",
    "Respond with a score from 0.0 to 1.0, where:\n",
    "- 1.0 = Every claim has support in context\n",
    "- 0.7-0.9 = Most claims supported (minor unsupported details)\n",
    "- 0.4-0.6 = Some claims unsupported\n",
    "- 0.0-0.3 = Many claims unsupported or contradictory\n",
    "\n",
    "Respond with ONLY a number between 0.0 and 1.0.\"\"\"\n",
    "        \n",
    "        response = self.client.messages.create(\n",
    "            model=\"claude-sonnet-4-20250514\",\n",
    "            max_tokens=10,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            score = float(response.content[0].text.strip())\n",
    "            return max(0.0, min(1.0, score))\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_relevance(self, \n",
    "                          query: str, \n",
    "                          generated_answer: str) -> float:\n",
    "        \"\"\"Score: Does the answer actually address the question?\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"You are evaluating answer relevance.\n",
    "\n",
    "Query: {query}\n",
    "Generated Answer: {generated_answer}\n",
    "\n",
    "Does the answer directly address the question asked?\n",
    "Respond with a score from 0.0 to 1.0, where:\n",
    "- 1.0 = Perfectly addresses the question\n",
    "- 0.7-0.9 = Mostly relevant (minor tangents)\n",
    "- 0.4-0.6 = Partially relevant\n",
    "- 0.0-0.3 = Off-topic or doesn't answer the question\n",
    "\n",
    "Respond with ONLY a number between 0.0 and 1.0.\"\"\"\n",
    "        \n",
    "        response = self.client.messages.create(\n",
    "            model=\"claude-sonnet-4-20250514\",\n",
    "            max_tokens=10,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            score = float(response.content[0].text.strip())\n",
    "            return max(0.0, min(1.0, score))\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_batch(self, \n",
    "                      eval_examples: List[EvalExample],\n",
    "                      rag_fn) -> AnswerMetrics:\n",
    "        \"\"\"Run full answer evaluation\"\"\"\n",
    "        \n",
    "        correctness_scores = []\n",
    "        faithfulness_scores = []\n",
    "        relevance_scores = []\n",
    "        \n",
    "        for example in eval_examples:\n",
    "            # Generate answer using your RAG system\n",
    "            answer, retrieved_docs = rag_fn(example.query)\n",
    "            \n",
    "            # Evaluate\n",
    "            c = self.evaluate_correctness(example.query, answer, example.ground_truth_answer)\n",
    "            f = self.evaluate_faithfulness(answer, retrieved_docs)\n",
    "            r = self.evaluate_relevance(example.query, answer)\n",
    "            \n",
    "            correctness_scores.append(c)\n",
    "            faithfulness_scores.append(f)\n",
    "            relevance_scores.append(r)\n",
    "        \n",
    "        return AnswerMetrics(\n",
    "            correctness=np.mean(correctness_scores),\n",
    "            faithfulness=np.mean(faithfulness_scores),\n",
    "            relevance=np.mean(relevance_scores),\n",
    "            num_queries=len(eval_examples)\n",
    "        )\n",
    "\n",
    "# Usage example\n",
    "# evaluator = AnswerEvaluator(anthropic_client)\n",
    "# metrics = evaluator.evaluate_batch(eval_examples, my_rag_function)\n",
    "# print(f\"Correctness: {metrics.correctness:.3f}\")\n",
    "# print(f\"Faithfulness: {metrics.faithfulness:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatencyBenchmark:\n",
    "    \"\"\"Measure latency distribution\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def measure(fn, queries: List[str], warmup: int = 3) -> LatencyMetrics:\n",
    "        \"\"\"Measure latency for a function\"\"\"\n",
    "        \n",
    "        # Warmup\n",
    "        for i in range(min(warmup, len(queries))):\n",
    "            fn(queries[i])\n",
    "        \n",
    "        # Actual measurement\n",
    "        latencies = []\n",
    "        for query in queries:\n",
    "            start = time.time()\n",
    "            fn(query)\n",
    "            latency = time.time() - start\n",
    "            latencies.append(latency * 1000)  # Convert to ms\n",
    "        \n",
    "        latencies = np.array(latencies)\n",
    "        \n",
    "        return LatencyMetrics(\n",
    "            p50=np.percentile(latencies, 50),\n",
    "            p95=np.percentile(latencies, 95),\n",
    "            p99=np.percentile(latencies, 99),\n",
    "            mean=np.mean(latencies)\n",
    "        )\n",
    "\n",
    "# Usage example\n",
    "# latency = LatencyBenchmark.measure(my_rag_function, test_queries)\n",
    "# print(f\"P95: {latency.p95:.0f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Synthetic Test Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticEvalGenerator:\n",
    "    \"\"\"Generate evaluation examples from documents\"\"\"\n",
    "    \n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "    \n",
    "    def generate_questions(self, \n",
    "                          document: str, \n",
    "                          doc_id: str,\n",
    "                          num_questions: int = 5) -> List[EvalExample]:\n",
    "        \"\"\"Generate questions from a document\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"You are generating test questions from a document.\n",
    "\n",
    "Document:\n",
    "{document}\n",
    "\n",
    "Generate {num_questions} questions that can be answered using ONLY this document.\n",
    "Include a mix of:\n",
    "- Simple factual questions (Who, What, When)\n",
    "- Reasoning questions (Why, How)\n",
    "- Questions about specific details\n",
    "\n",
    "For each question, provide:\n",
    "1. The question\n",
    "2. The answer (based on the document)\n",
    "\n",
    "Format as JSON:\n",
    "{{\n",
    "  \"questions\": [\n",
    "    {{\n",
    "      \"question\": \"What is...\",\n",
    "      \"answer\": \"According to the document...\"\n",
    "    }}\n",
    "  ]\n",
    "}}\"\"\"\n",
    "        \n",
    "        response = self.client.messages.create(\n",
    "            model=\"claude-sonnet-4-20250514\",\n",
    "            max_tokens=2000,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            data = json.loads(response.content[0].text)\n",
    "            examples = []\n",
    "            \n",
    "            for item in data[\"questions\"]:\n",
    "                examples.append(EvalExample(\n",
    "                    query=item[\"question\"],\n",
    "                    ground_truth_answer=item[\"answer\"],\n",
    "                    relevant_doc_ids=[doc_id]\n",
    "                ))\n",
    "            \n",
    "            return examples\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    def generate_eval_set(self, \n",
    "                         documents: List[Tuple[str, str]],  # (doc_id, text)\n",
    "                         questions_per_doc: int = 3) -> List[EvalExample]:\n",
    "        \"\"\"Generate full eval set from documents\"\"\"\n",
    "        \n",
    "        all_examples = []\n",
    "        \n",
    "        for doc_id, text in documents:\n",
    "            examples = self.generate_questions(text, doc_id, questions_per_doc)\n",
    "            all_examples.extend(examples)\n",
    "        \n",
    "        return all_examples\n",
    "\n",
    "# Usage example\n",
    "# generator = SyntheticEvalGenerator(anthropic_client)\n",
    "# eval_set = generator.generate_eval_set(my_documents)\n",
    "# print(f\"Generated {len(eval_set)} evaluation examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAG Pattern Implementations\n",
    "\n",
    "### Pattern 1: Naive RAG (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveRAG:\n",
    "    \"\"\"Basic vector search + context stuffing\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"naive_rag\"):\n",
    "        # Initialize vector DB\n",
    "        self.chroma_client = chromadb.Client()\n",
    "        \n",
    "        # Use OpenAI embeddings\n",
    "        self.embedding_fn = embedding_functions.OpenAIEmbeddingFunction(\n",
    "            api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "            model_name=\"text-embedding-3-small\"\n",
    "        )\n",
    "        \n",
    "        # Create collection\n",
    "        self.collection = self.chroma_client.create_collection(\n",
    "            name=collection_name,\n",
    "            embedding_function=self.embedding_fn,\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "        \n",
    "        self.llm_client = openai_client\n",
    "    \n",
    "    def index_documents(self, documents: List[Dict[str, str]]):\n",
    "        \"\"\"Index documents into vector DB\n",
    "        \n",
    "        documents: List of {\"id\": str, \"text\": str, \"metadata\": dict}\n",
    "        \"\"\"\n",
    "        ids = [doc[\"id\"] for doc in documents]\n",
    "        texts = [doc[\"text\"] for doc in documents]\n",
    "        metadatas = [doc.get(\"metadata\", {}) for doc in documents]\n",
    "        \n",
    "        self.collection.add(\n",
    "            ids=ids,\n",
    "            documents=texts,\n",
    "            metadatas=metadatas\n",
    "        )\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 5) -> Tuple[List[str], List[str]]:\n",
    "        \"\"\"Retrieve relevant documents\n",
    "        \n",
    "        Returns: (doc_ids, doc_texts)\n",
    "        \"\"\"\n",
    "        results = self.collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=top_k\n",
    "        )\n",
    "        \n",
    "        doc_ids = results[\"ids\"][0]\n",
    "        doc_texts = results[\"documents\"][0]\n",
    "        \n",
    "        return doc_ids, doc_texts\n",
    "    \n",
    "    def generate(self, query: str, context: List[str]) -> str:\n",
    "        \"\"\"Generate answer from context\"\"\"\n",
    "        \n",
    "        context_str = \"\\n\\n\".join([\n",
    "            f\"[Document {i+1}]\\n{doc}\" \n",
    "            for i, doc in enumerate(context)\n",
    "        ])\n",
    "        \n",
    "        prompt = f\"\"\"You are a helpful assistant. Answer the question based on the context provided.\n",
    "\n",
    "Context:\n",
    "{context_str}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        response = self.llm_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    def query(self, query: str, top_k: int = 5) -> Tuple[str, List[str]]:\n",
    "        \"\"\"End-to-end RAG pipeline\n",
    "        \n",
    "        Returns: (answer, retrieved_docs)\n",
    "        \"\"\"\n",
    "        # Retrieve\n",
    "        doc_ids, doc_texts = self.retrieve(query, top_k)\n",
    "        \n",
    "        # Generate\n",
    "        answer = self.generate(query, doc_texts)\n",
    "        \n",
    "        return answer, doc_texts\n",
    "\n",
    "# Usage example\n",
    "# rag = NaiveRAG()\n",
    "# rag.index_documents(my_documents)\n",
    "# answer, docs = rag.query(\"What is...?\")\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Parent Document Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParentDocumentRAG:\n",
    "    \"\"\"Search small chunks, return full parent documents\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 collection_name: str = \"parent_doc_rag\",\n",
    "                 chunk_size: int = 256,\n",
    "                 chunk_overlap: int = 50):\n",
    "        \n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        \n",
    "        # Vector DB for chunks\n",
    "        self.chroma_client = chromadb.Client()\n",
    "        self.embedding_fn = embedding_functions.OpenAIEmbeddingFunction(\n",
    "            api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "            model_name=\"text-embedding-3-small\"\n",
    "        )\n",
    "        self.collection = self.chroma_client.create_collection(\n",
    "            name=collection_name,\n",
    "            embedding_function=self.embedding_fn\n",
    "        )\n",
    "        \n",
    "        # Store parent documents\n",
    "        self.parent_docs = {}\n",
    "        \n",
    "        self.llm_client = openai_client\n",
    "    \n",
    "    def chunk_text(self, text: str, parent_id: str) -> List[Dict]:\n",
    "        \"\"\"Split text into overlapping chunks\"\"\"\n",
    "        chunks = []\n",
    "        words = text.split()\n",
    "        \n",
    "        for i in range(0, len(words), self.chunk_size - self.chunk_overlap):\n",
    "            chunk_words = words[i:i + self.chunk_size]\n",
    "            chunk_text = \" \".join(chunk_words)\n",
    "            \n",
    "            chunks.append({\n",
    "                \"id\": f\"{parent_id}_chunk_{len(chunks)}\",\n",
    "                \"text\": chunk_text,\n",
    "                \"parent_id\": parent_id\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def index_documents(self, documents: List[Dict[str, str]]):\n",
    "        \"\"\"Index documents with chunking\n",
    "        \n",
    "        documents: List of {\"id\": str, \"text\": str}\n",
    "        \"\"\"\n",
    "        all_chunks = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            # Store parent\n",
    "            self.parent_docs[doc[\"id\"]] = doc[\"text\"]\n",
    "            \n",
    "            # Create chunks\n",
    "            chunks = self.chunk_text(doc[\"text\"], doc[\"id\"])\n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        # Index chunks\n",
    "        if all_chunks:\n",
    "            self.collection.add(\n",
    "                ids=[c[\"id\"] for c in all_chunks],\n",
    "                documents=[c[\"text\"] for c in all_chunks],\n",
    "                metadatas=[{\"parent_id\": c[\"parent_id\"]} for c in all_chunks]\n",
    "            )\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 10) -> List[str]:\n",
    "        \"\"\"Retrieve parent documents based on chunk search\"\"\"\n",
    "        \n",
    "        # Search chunks\n",
    "        results = self.collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=top_k\n",
    "        )\n",
    "        \n",
    "        # Get unique parent IDs\n",
    "        parent_ids = []\n",
    "        seen = set()\n",
    "        \n",
    "        for metadata in results[\"metadatas\"][0]:\n",
    "            parent_id = metadata[\"parent_id\"]\n",
    "            if parent_id not in seen:\n",
    "                parent_ids.append(parent_id)\n",
    "                seen.add(parent_id)\n",
    "        \n",
    "        # Return full parent documents\n",
    "        parent_texts = [self.parent_docs[pid] for pid in parent_ids]\n",
    "        \n",
    "        return parent_texts[:5]  # Limit to top 5 parents\n",
    "    \n",
    "    def query(self, query: str) -> Tuple[str, List[str]]:\n",
    "        \"\"\"End-to-end RAG pipeline\"\"\"\n",
    "        \n",
    "        # Retrieve parent docs\n",
    "        parent_docs = self.retrieve(query)\n",
    "        \n",
    "        # Generate answer\n",
    "        context_str = \"\\n\\n\".join([\n",
    "            f\"[Document {i+1}]\\n{doc}\" \n",
    "            for i, doc in enumerate(parent_docs)\n",
    "        ])\n",
    "        \n",
    "        prompt = f\"\"\"Answer the question based on the context.\n",
    "\n",
    "Context:\n",
    "{context_str}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        response = self.llm_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "        return answer, parent_docs\n",
    "\n",
    "# Usage example\n",
    "# rag = ParentDocumentRAG(chunk_size=256)\n",
    "# rag.index_documents(my_documents)\n",
    "# answer, docs = rag.query(\"What is...?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 3: Corrective RAG (CRAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrectiveRAG:\n",
    "    \"\"\"RAG with relevance grading and fallback\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 collection_name: str = \"crag\",\n",
    "                 relevance_threshold: float = 0.6):\n",
    "        \n",
    "        self.relevance_threshold = relevance_threshold\n",
    "        \n",
    "        # Base RAG\n",
    "        self.base_rag = NaiveRAG(collection_name)\n",
    "        self.llm_client = anthropic_client\n",
    "    \n",
    "    def index_documents(self, documents: List[Dict[str, str]]):\n",
    "        \"\"\"Index documents\"\"\"\n",
    "        self.base_rag.index_documents(documents)\n",
    "    \n",
    "    def grade_document(self, query: str, document: str) -> float:\n",
    "        \"\"\"Grade document relevance to query\n",
    "        \n",
    "        Returns: Relevance score 0.0-1.0\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"You are a document grader.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Document:\n",
    "{document}\n",
    "\n",
    "Is this document relevant to answering the query?\n",
    "Respond with ONLY a score from 0.0 to 1.0 where:\n",
    "- 1.0 = Highly relevant, contains answer\n",
    "- 0.5-0.8 = Somewhat relevant\n",
    "- 0.0-0.4 = Not relevant\n",
    "\n",
    "Score:\"\"\"\n",
    "        \n",
    "        response = self.llm_client.messages.create(\n",
    "            model=\"claude-sonnet-4-20250514\",\n",
    "            max_tokens=10,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            score = float(response.content[0].text.strip())\n",
    "            return max(0.0, min(1.0, score))\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def web_search_fallback(self, query: str) -> str:\n",
    "        \"\"\"Fallback to web search\n",
    "        \n",
    "        In production, integrate with real web search API\n",
    "        \"\"\"\n",
    "        return f\"[Web search would be triggered for: {query}]\"\n",
    "    \n",
    "    def query(self, query: str, top_k: int = 10) -> Tuple[str, List[str]]:\n",
    "        \"\"\"CRAG pipeline with grading and fallback\"\"\"\n",
    "        \n",
    "        # Initial retrieval\n",
    "        doc_ids, doc_texts = self.base_rag.retrieve(query, top_k)\n",
    "        \n",
    "        # Grade each document\n",
    "        graded_docs = []\n",
    "        for doc in doc_texts:\n",
    "            score = self.grade_document(query, doc)\n",
    "            if score >= self.relevance_threshold:\n",
    "                graded_docs.append(doc)\n",
    "        \n",
    "        # Check if we have enough relevant docs\n",
    "        if len(graded_docs) < 2:\n",
    "            # Trigger fallback\n",
    "            fallback_result = self.web_search_fallback(query)\n",
    "            graded_docs.append(fallback_result)\n",
    "        \n",
    "        # Generate answer from verified docs\n",
    "        if graded_docs:\n",
    "            answer = self.base_rag.generate(query, graded_docs)\n",
    "        else:\n",
    "            answer = \"I don't have enough reliable information to answer this question.\"\n",
    "        \n",
    "        return answer, graded_docs\n",
    "\n",
    "# Usage example\n",
    "# rag = CorrectiveRAG(relevance_threshold=0.7)\n",
    "# rag.index_documents(my_documents)\n",
    "# answer, verified_docs = rag.query(\"What is...?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 4: HyDE (Hypothetical Document Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyDERAG:\n",
    "    \"\"\"Generate hypothetical answer, search with it\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"hyde_rag\"):\n",
    "        self.base_rag = NaiveRAG(collection_name)\n",
    "        self.llm_client = openai_client\n",
    "        \n",
    "        # Cache for common queries\n",
    "        self.hyde_cache = {}\n",
    "    \n",
    "    def index_documents(self, documents: List[Dict[str, str]]):\n",
    "        \"\"\"Index documents\"\"\"\n",
    "        self.base_rag.index_documents(documents)\n",
    "    \n",
    "    def generate_hypothetical_doc(self, query: str) -> str:\n",
    "        \"\"\"Generate hypothetical answer to query\"\"\"\n",
    "        \n",
    "        # Check cache\n",
    "        if query in self.hyde_cache:\n",
    "            return self.hyde_cache[query]\n",
    "        \n",
    "        prompt = f\"\"\"Write a detailed, factual answer to this question as if you were a technical document.\n",
    "Use formal language and technical vocabulary.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        response = self.llm_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        hypothetical_doc = response.choices[0].message.content\n",
    "        \n",
    "        # Cache it\n",
    "        self.hyde_cache[query] = hypothetical_doc\n",
    "        \n",
    "        return hypothetical_doc\n",
    "    \n",
    "    def query(self, query: str, top_k: int = 5) -> Tuple[str, List[str]]:\n",
    "        \"\"\"HyDE pipeline\"\"\"\n",
    "        \n",
    "        # Generate hypothetical document\n",
    "        hypothetical_doc = self.generate_hypothetical_doc(query)\n",
    "        \n",
    "        # Search using hypothetical doc\n",
    "        doc_ids, doc_texts = self.base_rag.retrieve(hypothetical_doc, top_k)\n",
    "        \n",
    "        # Generate real answer from real docs\n",
    "        answer = self.base_rag.generate(query, doc_texts)\n",
    "        \n",
    "        return answer, doc_texts\n",
    "\n",
    "# Usage example\n",
    "# rag = HyDERAG()\n",
    "# rag.index_documents(my_documents)\n",
    "# answer, docs = rag.query(\"How do I...?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 5: Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RerankedRAG:\n",
    "    \"\"\"Vector search + cross-encoder reranking\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 collection_name: str = \"reranked_rag\",\n",
    "                 reranker_model: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"):\n",
    "        \n",
    "        self.base_rag = NaiveRAG(collection_name)\n",
    "        \n",
    "        # Load reranker model\n",
    "        from sentence_transformers import CrossEncoder\n",
    "        self.reranker = CrossEncoder(reranker_model)\n",
    "    \n",
    "    def index_documents(self, documents: List[Dict[str, str]]):\n",
    "        \"\"\"Index documents\"\"\"\n",
    "        self.base_rag.index_documents(documents)\n",
    "    \n",
    "    def rerank(self, \n",
    "               query: str, \n",
    "               documents: List[str], \n",
    "               top_k: int = 5) -> List[str]:\n",
    "        \"\"\"Rerank documents using cross-encoder\"\"\"\n",
    "        \n",
    "        # Score each query-doc pair\n",
    "        pairs = [[query, doc] for doc in documents]\n",
    "        scores = self.reranker.predict(pairs)\n",
    "        \n",
    "        # Sort by score\n",
    "        doc_scores = list(zip(documents, scores))\n",
    "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k\n",
    "        reranked_docs = [doc for doc, score in doc_scores[:top_k]]\n",
    "        \n",
    "        return reranked_docs\n",
    "    \n",
    "    def query(self, \n",
    "              query: str, \n",
    "              retrieval_k: int = 50,\n",
    "              final_k: int = 5) -> Tuple[str, List[str]]:\n",
    "        \"\"\"Retrieve many, rerank to few\"\"\"\n",
    "        \n",
    "        # Retrieve many candidates\n",
    "        doc_ids, doc_texts = self.base_rag.retrieve(query, retrieval_k)\n",
    "        \n",
    "        # Rerank to top k\n",
    "        reranked_docs = self.rerank(query, doc_texts, final_k)\n",
    "        \n",
    "        # Generate answer\n",
    "        answer = self.base_rag.generate(query, reranked_docs)\n",
    "        \n",
    "        return answer, reranked_docs\n",
    "\n",
    "# Usage example\n",
    "# rag = RerankedRAG()\n",
    "# rag.index_documents(my_documents)\n",
    "# answer, docs = rag.query(\"What is...?\", retrieval_k=100, final_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_evaluation(rag_system, \n",
    "                       eval_examples: List[EvalExample],\n",
    "                       name: str = \"RAG System\"):\n",
    "    \"\"\"Run complete evaluation suite\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating: {name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # 1. Retrieval Evaluation\n",
    "    print(\"[1/3] Retrieval Evaluation...\")\n",
    "    \n",
    "    def retrieval_fn(query):\n",
    "        doc_ids, _ = rag_system.retrieve(query, top_k=10)\n",
    "        return doc_ids\n",
    "    \n",
    "    retrieval_eval = RetrievalEvaluator()\n",
    "    retrieval_metrics = retrieval_eval.evaluate(eval_examples, retrieval_fn)\n",
    "    \n",
    "    print(f\"  Precision@5:  {retrieval_metrics.precision_at_k[5]:.3f}\")\n",
    "    print(f\"  Recall@10:    {retrieval_metrics.recall_at_k[10]:.3f}\")\n",
    "    print(f\"  MRR:          {retrieval_metrics.mrr:.3f}\")\n",
    "    \n",
    "    # 2. Answer Evaluation (sample for speed)\n",
    "    print(\"\\n[2/3] Answer Evaluation (sample of 20)...\")\n",
    "    \n",
    "    sample_examples = eval_examples[:20]\n",
    "    \n",
    "    def rag_fn(query):\n",
    "        return rag_system.query(query)\n",
    "    \n",
    "    answer_eval = AnswerEvaluator(anthropic_client)\n",
    "    answer_metrics = answer_eval.evaluate_batch(sample_examples, rag_fn)\n",
    "    \n",
    "    print(f\"  Correctness:  {answer_metrics.correctness:.3f}\")\n",
    "    print(f\"  Faithfulness: {answer_metrics.faithfulness:.3f}\")\n",
    "    print(f\"  Relevance:    {answer_metrics.relevance:.3f}\")\n",
    "    \n",
    "    # 3. Latency Benchmark\n",
    "    print(\"\\n[3/3] Latency Benchmark...\")\n",
    "    \n",
    "    test_queries = [ex.query for ex in eval_examples[:30]]\n",
    "    \n",
    "    def query_fn(q):\n",
    "        return rag_system.query(q)\n",
    "    \n",
    "    latency_metrics = LatencyBenchmark.measure(query_fn, test_queries)\n",
    "    \n",
    "    print(f\"  P50:  {latency_metrics.p50:.0f}ms\")\n",
    "    print(f\"  P95:  {latency_metrics.p95:.0f}ms\")\n",
    "    print(f\"  P99:  {latency_metrics.p99:.0f}ms\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Summary\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Pass/Fail checks\n",
    "    checks = [\n",
    "        (\"Precision@5 > 0.6\", retrieval_metrics.precision_at_k[5] > 0.6),\n",
    "        (\"Recall@10 > 0.8\", retrieval_metrics.recall_at_k[10] > 0.8),\n",
    "        (\"MRR > 0.5\", retrieval_metrics.mrr > 0.5),\n",
    "        (\"Correctness > 0.85\", answer_metrics.correctness > 0.85),\n",
    "        (\"Faithfulness > 0.9\", answer_metrics.faithfulness > 0.9),\n",
    "        (\"P95 Latency < 3000ms\", latency_metrics.p95 < 3000),\n",
    "    ]\n",
    "    \n",
    "    for check_name, passed in checks:\n",
    "        status = \"âœ“ PASS\" if passed else \"âœ— FAIL\"\n",
    "        print(f\"  {status}  {check_name}\")\n",
    "    \n",
    "    all_passed = all(passed for _, passed in checks)\n",
    "    \n",
    "    if all_passed:\n",
    "        print(\"\\nðŸŽ‰ All checks passed! System ready for production.\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  Some checks failed. Review metrics before deploying.\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\\n\")\n",
    "\n",
    "# Usage example\n",
    "# my_rag = NaiveRAG()\n",
    "# my_rag.index_documents(documents)\n",
    "# run_full_evaluation(my_rag, eval_examples, \"Naive RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Example: End-to-End Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example workflow\n",
    "\n",
    "# Step 1: Prepare documents\n",
    "sample_documents = [\n",
    "    {\n",
    "        \"id\": \"doc_1\",\n",
    "        \"text\": \"Python is a high-level programming language. It was created by Guido van Rossum and released in 1991.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_2\",\n",
    "        \"text\": \"Machine learning is a subset of artificial intelligence. It focuses on building systems that learn from data.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_3\",\n",
    "        \"text\": \"RAG (Retrieval Augmented Generation) combines retrieval systems with language models to provide grounded answers.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Step 2: Generate eval set\n",
    "generator = SyntheticEvalGenerator(anthropic_client)\n",
    "eval_examples = generator.generate_eval_set(\n",
    "    [(doc[\"id\"], doc[\"text\"]) for doc in sample_documents],\n",
    "    questions_per_doc=2\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(eval_examples)} eval examples\")\n",
    "\n",
    "# Step 3: Test multiple RAG patterns\n",
    "patterns = [\n",
    "    (\"Naive RAG\", NaiveRAG(\"test_naive\")),\n",
    "    (\"Parent Doc RAG\", ParentDocumentRAG(\"test_parent\")),\n",
    "    (\"CRAG\", CorrectiveRAG(\"test_crag\")),\n",
    "]\n",
    "\n",
    "for name, rag_system in patterns:\n",
    "    # Index documents\n",
    "    rag_system.index_documents(sample_documents)\n",
    "    \n",
    "    # Run evaluation\n",
    "    run_full_evaluation(rag_system, eval_examples, name)\n",
    "\n",
    "# Step 4: Compare and choose\n",
    "print(\"\\nCompare the results above to choose the best pattern for your use case.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Production Monitoring Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionMonitor:\n",
    "    \"\"\"Track production metrics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.latencies = []\n",
    "        self.errors = []\n",
    "        self.null_responses = 0\n",
    "        self.total_queries = 0\n",
    "        self.cache_hits = 0\n",
    "        self.user_feedback = {\"positive\": 0, \"negative\": 0}\n",
    "    \n",
    "    def log_query(self, \n",
    "                  latency_ms: float, \n",
    "                  was_cached: bool = False,\n",
    "                  was_null: bool = False,\n",
    "                  error: Optional[str] = None):\n",
    "        \"\"\"Log a single query\"\"\"\n",
    "        \n",
    "        self.total_queries += 1\n",
    "        self.latencies.append(latency_ms)\n",
    "        \n",
    "        if was_cached:\n",
    "            self.cache_hits += 1\n",
    "        \n",
    "        if was_null:\n",
    "            self.null_responses += 1\n",
    "        \n",
    "        if error:\n",
    "            self.errors.append(error)\n",
    "    \n",
    "    def log_feedback(self, is_positive: bool):\n",
    "        \"\"\"Log user feedback\"\"\"\n",
    "        if is_positive:\n",
    "            self.user_feedback[\"positive\"] += 1\n",
    "        else:\n",
    "            self.user_feedback[\"negative\"] += 1\n",
    "    \n",
    "    def get_metrics(self) -> Dict:\n",
    "        \"\"\"Get current metrics\"\"\"\n",
    "        \n",
    "        if not self.latencies:\n",
    "            return {}\n",
    "        \n",
    "        latencies = np.array(self.latencies)\n",
    "        \n",
    "        total_feedback = sum(self.user_feedback.values())\n",
    "        satisfaction = self.user_feedback[\"positive\"] / total_feedback if total_feedback > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"total_queries\": self.total_queries,\n",
    "            \"error_rate\": len(self.errors) / self.total_queries,\n",
    "            \"null_response_rate\": self.null_responses / self.total_queries,\n",
    "            \"cache_hit_rate\": self.cache_hits / self.total_queries,\n",
    "            \"latency_p50\": np.percentile(latencies, 50),\n",
    "            \"latency_p95\": np.percentile(latencies, 95),\n",
    "            \"latency_p99\": np.percentile(latencies, 99),\n",
    "            \"user_satisfaction\": satisfaction,\n",
    "        }\n",
    "    \n",
    "    def check_alerts(self) -> List[str]:\n",
    "        \"\"\"Check if any alert thresholds are breached\"\"\"\n",
    "        \n",
    "        alerts = []\n",
    "        metrics = self.get_metrics()\n",
    "        \n",
    "        if metrics.get(\"error_rate\", 0) > 0.05:\n",
    "            alerts.append(\"ðŸš¨ CRITICAL: Error rate > 5%\")\n",
    "        \n",
    "        if metrics.get(\"latency_p95\", 0) > 5000:\n",
    "            alerts.append(\"ðŸš¨ CRITICAL: P95 latency > 5s\")\n",
    "        \n",
    "        if metrics.get(\"null_response_rate\", 0) > 0.10:\n",
    "            alerts.append(\"âš ï¸  WARNING: Null response rate > 10%\")\n",
    "        \n",
    "        if metrics.get(\"cache_hit_rate\", 0) < 0.20:\n",
    "            alerts.append(\"âš ï¸  WARNING: Cache hit rate < 20%\")\n",
    "        \n",
    "        return alerts\n",
    "\n",
    "# Usage example\n",
    "# monitor = ProductionMonitor()\n",
    "# \n",
    "# # In your RAG endpoint:\n",
    "# start = time.time()\n",
    "# answer, docs = rag.query(user_query)\n",
    "# latency = (time.time() - start) * 1000\n",
    "# \n",
    "# monitor.log_query(\n",
    "#     latency_ms=latency,\n",
    "#     was_null=(answer == \"I don't know\")\n",
    "# )\n",
    "# \n",
    "# # Check for alerts\n",
    "# alerts = monitor.check_alerts()\n",
    "# for alert in alerts:\n",
    "#     print(alert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Quick Start Guide\n",
    "\n",
    "**For first-time users**: Start here and follow the steps.\n",
    "\n",
    "```python\n",
    "# 1. Prepare your data\n",
    "documents = [\n",
    "    {\"id\": \"doc_1\", \"text\": \"Your first document...\"},\n",
    "    {\"id\": \"doc_2\", \"text\": \"Your second document...\"},\n",
    "]\n",
    "\n",
    "# 2. Start with Naive RAG\n",
    "rag = NaiveRAG()\n",
    "rag.index_documents(documents)\n",
    "\n",
    "# 3. Generate eval set\n",
    "generator = SyntheticEvalGenerator(anthropic_client)\n",
    "eval_set = generator.generate_eval_set(\n",
    "    [(d[\"id\"], d[\"text\"]) for d in documents]\n",
    ")\n",
    "\n",
    "# 4. Run evaluation\n",
    "run_full_evaluation(rag, eval_set, \"Naive RAG\")\n",
    "\n",
    "# 5. If metrics fail:\n",
    "# - Precision@5 < 0.6 â†’ Try ParentDocumentRAG or tune chunk sizes\n",
    "# - Faithfulness < 0.9 â†’ Try CorrectiveRAG\n",
    "# - Recall@10 < 0.8 â†’ Try HyDE or better chunking\n",
    "# - MRR < 0.5 â†’ Add reranking\n",
    "\n",
    "# 6. Deploy with monitoring\n",
    "monitor = ProductionMonitor()\n",
    "# ... integrate into your API\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
